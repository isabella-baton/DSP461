{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oZOWWObz2ka"
      },
      "source": [
        "# Task for Today  \n",
        "\n",
        "***\n",
        "\n",
        "## Interview Success Prediction  \n",
        "\n",
        "Given *data about resumes*, let's try to predict whether a candidate will **pass their interview** based on their resume.\n",
        "\n",
        "We will use three models to make our predictions and PCA for dimensionality reduction to make our predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9JUBsD3z2ke"
      },
      "source": [
        "# Getting Started"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVo5spMXz2kg",
        "outputId": "594a2f2e-abe0-4555-acae-ce74ffd82ae4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/vingkan/strategeion-resume-skills/versions/2\n",
            "Files in dataset directory: ['fairness.py', 'resumes_development.csv', 'PARiS.pickle', 'skills.txt', 'resumes_pilot.csv']\n"
          ]
        }
      ],
      "source": [
        "# ML modules\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# kaggle api\n",
        "import os\n",
        "import kagglehub\n",
        "import shutil\n",
        "\n",
        "# download dataset from kaggle\n",
        "path = kagglehub.dataset_download(\"vingkan/strategeion-resume-skills\")\n",
        "print(\"Path to dataset files:\", path)\n",
        "files = os.listdir(path)\n",
        "print(\"Files in dataset directory:\", files)\n",
        "\n",
        "# create dataframes\n",
        "dev_data = pd.read_csv(os.path.join(path, 'resumes_development.csv'))\n",
        "pilot_data = pd.read_csv(os.path.join(path, 'resumes_pilot.csv'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count1 = 0\n",
        "count2 = 0\n",
        "count3 = 0\n",
        "count4 = 0\n",
        "count5 = 0\n",
        "count6 = 0\n",
        "count7 = 0\n",
        "my_data = pilot_data\n",
        "for i in range(len(my_data)):\n",
        "  row = my_data.iloc[i]\n",
        "  if row['Female']:\n",
        "    count1 += 1\n",
        "  if row['URM']:\n",
        "    count2 += 1\n",
        "  if row['Disability']:\n",
        "    count3 += 1\n",
        "  if row['Female'] and row['URM']:\n",
        "    count4 += 1\n",
        "  if row['Female'] and row['Disability']:\n",
        "    count5 += 1\n",
        "  if row['URM'] and row['Disability']:\n",
        "    count6 += 1\n",
        "  if row['Female'] and row['URM'] and row['Disability']:\n",
        "    count7 += 1\n",
        "print(count1, count2, count3, count4, count5, count6, count7)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5lwIuD7X0VS",
        "outputId": "59372f53-4e0b-459d-cf45-1682c8b1cb48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "976 930 872 901 763 791 762\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpzBpBWpz2kg"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8TX-W6Tz2kh"
      },
      "outputs": [],
      "source": [
        "# data transformations\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "def preprocess_inputs(data, seed=1):\n",
        "    df = data.copy()\n",
        "\n",
        "    # drop index column\n",
        "    df = df.drop('Unnamed: 0', axis=1)\n",
        "\n",
        "    # split df into X, y, and Hara\n",
        "    y = df['Interview']\n",
        "    X = df.drop('Interview', axis=1)\n",
        "\n",
        "    # train-test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True, random_state=seed)\n",
        "\n",
        "    # scale X\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(X_train)\n",
        "    X_train_scaled = pd.DataFrame(scaler.transform(X_train), index=X_train.index, columns=X_train.columns)\n",
        "    X_test_scaled = pd.DataFrame(scaler.transform(X_test), index=X_test.index, columns=X_test.columns)\n",
        "\n",
        "    return X_train_scaled, X_test_scaled, y_train, y_test, X_train, X_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGtDw64AG6oT"
      },
      "source": [
        "# Measuring bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9jthoaVG3oB"
      },
      "outputs": [],
      "source": [
        "def bias_loss(data):\n",
        "\n",
        "  prot_feats = ['Female', 'URM', 'Disability']\n",
        "  unprot_feat = 'Veteran'\n",
        "\n",
        "  # init state\n",
        "  sum_loss = 0\n",
        "  loss_n = 0\n",
        "  output = {'Female': {\n",
        "              'Veteran': {'pos_rate': 0, 'neg_rate': 0, 'rate_diff': 0},\n",
        "              'Civilian': {'pos_rate': 0, 'neg_rate': 0, 'rate_diff': 0}},\n",
        "            'URM': {\n",
        "              'Veteran': {'pos_rate': 0, 'neg_rate': 0, 'rate_diff': 0},\n",
        "              'Civilian': {'pos_rate': 0, 'neg_rate': 0, 'rate_diff': 0}},\n",
        "            'Disability': {\n",
        "              'Veteran': {'pos_rate': 0, 'neg_rate': 0, 'rate_diff': 0},\n",
        "              'Civilian': {'pos_rate': 0, 'neg_rate': 0, 'rate_diff': 0}}}\n",
        "\n",
        "  # iterate\n",
        "  for prot_feat in prot_feats:\n",
        "    for unprot_name, unprot_value in [('Veteran', 1), ('Civilian', 0)]:\n",
        "      total = data[( data[unprot_feat] == unprot_value)]\n",
        "      # get matching applicants\n",
        "      pos_total = total[( total[prot_feat] == 1 )]\n",
        "      neg_total = total[( total[prot_feat] == 0 )]\n",
        "\n",
        "      # get matching accepted applicants\n",
        "      pos_hired = pos_total[( pos_total['Interview'] == 1 )]\n",
        "      neg_hired = neg_total[( neg_total['Interview'] == 1 )]\n",
        "\n",
        "      # get lengths\n",
        "      pos_total = len(pos_total)\n",
        "      neg_total = len(neg_total)\n",
        "      pos_hired = len(pos_hired)\n",
        "      neg_hired = len(neg_hired)\n",
        "\n",
        "      # get loss for this feature\n",
        "      pos_rate = pos_hired / pos_total # female acceptance rate\n",
        "      neg_rate = neg_hired / neg_total # male acceptance rate\n",
        "      rate_diff = neg_rate - pos_rate\n",
        "      sum_loss += rate_diff\n",
        "      loss_n += 1\n",
        "\n",
        "      output[prot_feat][unprot_name]['pos_rate'] = pos_rate\n",
        "      output[prot_feat][unprot_name]['neg_rate'] = neg_rate\n",
        "      output[prot_feat][unprot_name]['rate_diff'] = rate_diff\n",
        "\n",
        "  return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZip_le_fiNc"
      },
      "source": [
        "# Custom Wrapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5jqUp6MrfmZW"
      },
      "outputs": [],
      "source": [
        "# base classifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "class UnbiasedRegression():\n",
        "  base_model = None\n",
        "  fem_civ_w = 0.38\n",
        "\n",
        "  def __init__(self, random_state):\n",
        "     self.base_model = LogisticRegression(random_state=random_state)\n",
        "\n",
        "  def fit(self, X_train, y_train):\n",
        "    self.base_model.fit(X_train, y_train)\n",
        "\n",
        "  def predict(self, X_test_scaled, X_test):\n",
        "    proba = self.base_model.predict_proba(X_test_scaled)[:, 1]\n",
        "    for i in range(len(X_test)):\n",
        "      row = X_test.iloc[i]\n",
        "      if row['Female'] and not row['URM'] and not row['Disability']:\n",
        "        proba[i] = 1\n",
        "    return np.clip(proba, 0, 1).round(0)\n",
        "\n",
        "  def score(self, X_test_scaled, y_test, X_test):\n",
        "    return np.mean((self.predict(X_test_scaled, X_test) == y_test).astype(int))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iot6HnEz2ki"
      },
      "source": [
        "# Training/Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umahB_rSz2ki",
        "outputId": "2701d625-690a-4870-d5c7-2dbf77e9c4b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 94.09%\n",
            "Female Veteran rate_diff 90.48% - 71.70% = 18.78%\n",
            "Female Civilian rate_diff 11.11% - 12.90% = -1.79%\n",
            "URM Veteran rate_diff 89.47% - 72.73% = 16.75%\n",
            "URM Civilian rate_diff 14.29% - 3.57% = 10.71%\n",
            "Disability Veteran rate_diff 100.00% - 73.44% = 26.56%\n",
            "Disability Civilian rate_diff 12.77% - 5.56% = 7.21%\n"
          ]
        }
      ],
      "source": [
        "trials = 1\n",
        "sum_acc = 0\n",
        "sum_bias = {'Female': {\n",
        "              'Veteran': {'pos_rate': 0, 'neg_rate': 0, 'rate_diff': 0},\n",
        "              'Civilian': {'pos_rate': 0, 'neg_rate': 0, 'rate_diff': 0}},\n",
        "            'URM': {\n",
        "              'Veteran': {'pos_rate': 0, 'neg_rate': 0, 'rate_diff': 0},\n",
        "              'Civilian': {'pos_rate': 0, 'neg_rate': 0, 'rate_diff': 0}},\n",
        "            'Disability': {\n",
        "              'Veteran': {'pos_rate': 0, 'neg_rate': 0, 'rate_diff': 0},\n",
        "              'Civilian': {'pos_rate': 0, 'neg_rate': 0, 'rate_diff': 0}}}\n",
        "\n",
        "for r_state in range(trials):\n",
        "\n",
        "  # preprocess\n",
        "  X_train_scaled, X_test_scaled, y_train, y_test, X_train, X_test  = preprocess_inputs(dev_data, r_state)\n",
        "\n",
        "  # train model\n",
        "  model = UnbiasedRegression(random_state=r_state)\n",
        "  model.fit(X_train_scaled, y_train)\n",
        "\n",
        "  # find accuracy\n",
        "  acc = model.score(X_test_scaled, y_test, X_test)\n",
        "  sum_acc += acc\n",
        "\n",
        "  # find bias\n",
        "  y_pred = model.predict(X_test_scaled, X_test)\n",
        "  X_test['Interview'] = y_pred\n",
        "  bias = bias_loss(X_test)\n",
        "\n",
        "  for prot_feat in ['Female', 'URM', 'Disability']:\n",
        "    for vet_status, _ in [('Veteran', 1), ('Civilian', 0)]:\n",
        "      for rate in ['pos_rate', 'neg_rate', 'rate_diff']:\n",
        "        sum_bias[prot_feat][vet_status][rate] += bias[prot_feat][vet_status][rate]\n",
        "\n",
        "avg_acc = sum_acc / trials * 100\n",
        "print('Accuracy: {:.2f}%'.format(avg_acc))\n",
        "\n",
        "for prot_feat in ['Female', 'URM', 'Disability']:\n",
        "  for vet_status in ['Veteran', 'Civilian']:\n",
        "    group = sum_bias[prot_feat][vet_status]\n",
        "    pos_rate = group['pos_rate'] / trials * 100\n",
        "    neg_rate = group['neg_rate'] / trials * 100\n",
        "    rate_diff = group['rate_diff'] / trials * 100\n",
        "    print(prot_feat, vet_status, rate, '{:.2f}% - {:.2f}% = {:.2f}%'.format(neg_rate, pos_rate, rate_diff))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NA1vm-Sz2kj"
      },
      "source": [
        "# Training/Results With Dimensionality Reduction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nToD5YShz2kj"
      },
      "outputs": [],
      "source": [
        "n_components = 5\n",
        "\n",
        "pca = PCA(n_components=n_components)\n",
        "pca.fit(X_train)\n",
        "\n",
        "X_train_reduced = pd.DataFrame(pca.transform(X_train), index=X_train.index, columns=[\"PC\" + str(i) for i in range(1, n_components + 1)])\n",
        "X_test_reduced = pd.DataFrame(pca.transform(X_test), index=X_test.index, columns=[\"PC\" + str(i) for i in range(1, n_components + 1)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EwOo7sNJz2kj"
      },
      "outputs": [],
      "source": [
        "X_train_reduced"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-gJ0N1-z2kj"
      },
      "outputs": [],
      "source": [
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(),\n",
        "    \"      Decision Tree\": DecisionTreeClassifier(),\n",
        "    \"      Random Forest\": RandomForestClassifier()\n",
        "}\n",
        "\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train_reduced, y_train)\n",
        "    print(name + \" trained.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56CV8-Ncz2kj"
      },
      "outputs": [],
      "source": [
        "for name, model in models.items():\n",
        "    print(name + \" Accuracy: {:.2f}%\".format(model.score(X_test_reduced, y_test) * 100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1KYVxNvz2kk"
      },
      "source": [
        "# Data Every Day  \n",
        "\n",
        "This notebook is featured on Data Every Day, a YouTube series where I train models on a new dataset each day.  \n",
        "\n",
        "***\n",
        "\n",
        "Check it out!  \n",
        "https://youtu.be/BhlR-kHxc3E"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "datasetId": 102606,
          "sourceId": 243457,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30066,
      "isGpuEnabled": false,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}